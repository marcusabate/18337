{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will start to dig into what scientific machine learning is all about\n",
    "by looking at physics-informed neural networks. Let's start by understanding\n",
    "what a neural network really is, why they are used, and what kinds of problems\n",
    "that they solve, and then we will use this understanding of a neural network\n",
    "to see how to solve ordinary differential equations with neural networks.\n",
    "For there, we will use this method to regularize neural networks with physical\n",
    "equations, the aforementioned physics-informed neural network, and see how to\n",
    "define neural network architectures that satisfy physical constraints to improve\n",
    "the training process.\n",
    "\n",
    "## Getting Started with Machine Learning: Adding Flux\n",
    "\n",
    "To add Flux.jl we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "]add Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To then use the package we will then use the `using` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer to namespace all commands (like is normally done in Python, i.e.\n",
    "`Flux.gradient` instead of `gradient`), you can use the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the installation and precompilation of these packages will occur at\n",
    "the `add` and first `using` phases, so they may take awhile (subsequent uses\n",
    "will utilize the precompiled form and take a lot less time!)\n",
    "\n",
    "## What is a Neural Network?\n",
    "\n",
    "A neural network is a function:\n",
    "\n",
    "```math\n",
    "\\text{NN}(x) = W_3\\sigma_2(W_2\\sigma_1(W_1x + b_1) + b_2) + b_3\n",
    "```\n",
    "\n",
    "where we can change the number of layers (`(W_i,b_i)`) as necesary. Let's assume\n",
    "we want to approximate some $R^{10} \\rightarrow R^5$ function. To do this we need\n",
    "to make sure that we start with 10 inputs and arrive at 5 outputs. If we want a\n",
    "bigger middle layer for example, we can do something like (10,32,32,5). Size changing\n",
    "occurs at the site of the matrix multiplication, which means that we want a\n",
    "32x10 matrix, then a 32x32 matrix, and finally a 5x32 matrix. This neural network\n",
    "would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Array{Float64,1},1}:\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = [randn(32,10),randn(32,32),randn(5,32)]\n",
    "b = [zeros(32),zeros(32),zeros(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " -4.036382039445315\n",
       "  8.075045005877323\n",
       "  1.2791099995666007\n",
       " -0.6695665785285623\n",
       "  3.6861117722359475"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleNN(x) = W[3]*tanh.(W[2]*tanh.(W[1]*x + b[1]) + b[2]) + b[3]\n",
    "simpleNN(rand(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our direct definition of a neural network. Notice that we choose to use\n",
    "`tanh` as our **activation function** between the layers.\n",
    "\n",
    "### Defining Neural Networks with Flux.jl\n",
    "\n",
    "One of the main deep learning libraries in Julia is Flux.jl. Flux is an interesting\n",
    "library for scientific machine learning because it is built on top of language-wide\n",
    "**automatic differentiation** libraries, giving rise to a programming paradigm\n",
    "known as **differentiable programming**, which means that one can write a program\n",
    "in a manner that it has easily accessible fast derivatives. However, due to being\n",
    "built on a differentiable programming base, the underlying functionality is\n",
    "simply standard Julia code,\n",
    "\n",
    "To learn how to use the library, consult the documentation. A Google search\n",
    "will bring up the [Flux.jl Github repository](https://github.com/FluxML/Flux.jl).\n",
    "From there, the blue link on the README brings you to\n",
    "[the package documentation](https://fluxml.ai/Flux.jl/stable/). This is common\n",
    "through Julia so it's a good habit to learn!\n",
    "\n",
    "In the documentation you will find that the way a neural network is defined\n",
    "is through a `Chain` of layers. A `Dense` layer is the kind we defined above,\n",
    "which is given by an input size, an output size, and an activation function.\n",
    "For example, the following recreates the neural network that we had above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float32,1}:\n",
       " -0.63175404\n",
       " -0.07461834\n",
       "  0.13411549\n",
       " -0.42931926\n",
       "  0.1746519"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "NN2 = Chain(Dense(10,32,tanh),\n",
    "           Dense(32,32,tanh),\n",
    "           Dense(32,5))\n",
    "NN2(rand(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Flux.jl as a library is written in pure Julia, which means that every\n",
    "piece of this syntax is just sugar over some Julia code that we can specialize\n",
    "ourselves (this is the advantage of having a language fast enough for the\n",
    "implementation of the library and the use of the library!)\n",
    "\n",
    "For example, the activation function is just a scalar Julia function. If we wanted\n",
    "to replace it by something like the quadratic function, we can just use an\n",
    "**anonymous function** to define the scalar function we would like to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float32,1}:\n",
       "  0.049874395\n",
       " -0.115991406\n",
       " -0.42606148\n",
       "  0.035869017\n",
       " -0.255021"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN3 = Chain(Dense(10,32,x->x^2),\n",
    "            Dense(32,32,x->max(0,x)),\n",
    "            Dense(32,5))\n",
    "NN3(rand(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second activation function there is what's known as a `relu`. A `relu` can\n",
    "be good to use because it's an exceptionally operation and satisfies a form of\n",
    "the UAT. However, a downside is that its derivative is not continuous, which\n",
    "could impact the numerical properties of some algorithms, and thus it's widely\n",
    "used throughout standard machine learning but we'll see reasons why it may be\n",
    "disadvantageous in some cases in scientific machine learning.\n",
    "\n",
    "### Digging into the Construction of a Neural Network Library\n",
    "\n",
    "Again, as mentioned before, this neural network `NN2` is simply a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simpleNN (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleNN(x) = W[3]*tanh.(W[2]*tanh.(W[1]*x + b[1]) + b[2]) + b[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig into the library and see how that's represented and really understand\n",
    "the construction of a deep learning library. First, let's figure out where\n",
    "`Dense` comes from and what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "(::<b>Type{Dense}</b>)(in::<b>Integer</b>, out::<b>Integer</b>, σ; <i>initW, initb</i>) in Flux at <a href=\"file:///home/marcus/.julia/packages/Flux/05b38/src/layers/basic.jl\" target=\"_blank\">/home/marcus/.julia/packages/Flux/05b38/src/layers/basic.jl:114</a>"
      ],
      "text/plain": [
       "(::Type{Dense})(in::Integer, out::Integer, σ; initW, initb) in Flux at /home/marcus/.julia/packages/Flux/05b38/src/layers/basic.jl:114"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using InteractiveUtils\n",
    "@which Dense(10,32,tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go to that spot of the documentation, we find the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "cannot assign a value to variable Flux.Dense from module Main",
     "output_type": "error",
     "traceback": [
      "cannot assign a value to variable Flux.Dense from module Main",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[9]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "struct Dense{F,S<:AbstractArray,T<:AbstractArray}\n",
    "  W::S\n",
    "  b::T\n",
    "  σ::F\n",
    "end\n",
    "\n",
    "function Dense(in::Integer, out::Integer, σ = identity;\n",
    "               initW = glorot_uniform, initb = zeros)\n",
    "  return Dense(initW(out, in), initb(out), σ)\n",
    "end\n",
    "\n",
    "function (a::Dense)(x::AbstractArray)\n",
    "  W, b, σ = a.W, a.b, a.σ\n",
    "  σ.(W*x .+ b)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, `Dense` defines a struct in Julia. This struct just holds a weight matrix\n",
    "`W`, a bias vector `b`, and an activation function `σ`. The function called\n",
    "`Dense` is what's known as an **outer constructor** which defines how the\n",
    "`Dense` type is built. If you give it two integers (and optionally an activation\n",
    "function which defaults to `identity`), then what it will do is take random\n",
    "initial `W` and `b` matrices (according to the `glorot_uniform` distribution for\n",
    "`W` and `zeros` for `b`), and then it will build the type with those matrices.\n",
    "\n",
    "The last portion might be new. This is known as a **callable struct**, or a\n",
    "functor. It defines the dispatch for how calls work on the struct. As a quick\n",
    "demonstration, let's define a type `A` with a field `x`, and then make instances\n",
    "of `A` be the function `x+y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct A\n",
    "  x\n",
    "end\n",
    "\n",
    "(a::A)(y) = a.x+y\n",
    "a = A(2)\n",
    "a(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're familiar with object-oriented programming, this is similar to using\n",
    "an object in a way that references the `self`, though it's a bit more general\n",
    "due to allowing dispatching, i.e. this can then dependent on the input types\n",
    "as well.\n",
    "\n",
    "So let's look at that `Dense` call with this in mind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (a::Dense)(x::AbstractArray)\n",
    "  W, b, σ = a.W, a.b, a.σ\n",
    "  σ.(W*x .+ b)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that `Dense` is a function that takes in an `x` and computes\n",
    "`σ.(W*x.+b)`, which is precisely how we defined the layer before! To see that\n",
    "this is just a function, let's call it directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Dense(32,32,tanh)\n",
    "f(rand(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So okay, `Dense` objects are just functions that have weight and bias matrices\n",
    "inside of them. Now what does `Chain` do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@which Chain(1,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Chain{T<:Tuple}\n",
    "  layers::T\n",
    "  Chain(xs...) = new{typeof(xs)}(xs)\n",
    "end\n",
    "\n",
    "applychain(::Tuple{}, x) = x\n",
    "applychain(fs::Tuple, x) = applychain(tail(fs), first(fs)(x))\n",
    "\n",
    "(c::Chain)(x) = applychain(c.layers, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now dig into this. The `...` is known that the **slurp operator**, which\n",
    "allows for \"slurping up\" multiple arguments into a single object `xs`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurper(xs...) = @show xs\n",
    "slurper(1,2,3,4,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that slurps the inputs up into a `Tuple`, which is an immutable data store.\n",
    "(Note: Tuples are stack-allocated if inferred and is the internal data store of\n",
    "the compiler itself, and compiler inference can know exactly the size and the type\n",
    "of each individual object, so this does not have an overhead if fully inferred).\n",
    "\n",
    "The function `Chain(xs...) = new{typeof(xs)}(xs)` is an **inner constructor**\n",
    "which builds a new instance of `Chain` where `layers` is a tuple of the inputs.\n",
    "This means that in our case where we put a bunch of `Dense` inside of there,\n",
    "`layers` is a tuple of functions. What does `Chain` do? Let's look at its call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(c::Chain)(x) = applychain(c.layers, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes the tuple of functions and then does `applychain` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applychain(::Tuple{}, x) = x\n",
    "applychain(fs::Tuple, x) = applychain(tail(fs), first(fs)(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`applychain` is a recursive function which applies the first element of the tuple\n",
    "onto `x`, then it calls `applychain` to call the second function onto `x`, repeatedly\n",
    "until there are no more functions in which case it returns `x`. This means that\n",
    "`applychain` is simply doing `h(g(f(x)))` on the tuple of functions `(f,g,h)`!\n",
    "We can thus see that this library function is exactly equivalent to the neural\n",
    "network we defined by hand, just put together in a different form to give a\n",
    "nice user interface.\n",
    "\n",
    "#### Detail: Recursion?\n",
    "\n",
    "But there's one more detail... why recursion? If you define a function, look at\n",
    "its type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff(x,y) = 2x+y\n",
    "typeof(ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that its type is simply `typeof(ff)` which is unique to the function,\n",
    "i.e. every single function is its own struct. In fact, given what we just learned,\n",
    "it wouldn't be a surprise to learn that is exactly what a function is in Julia!\n",
    "A function definition lowers at the parser level to something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ff2 <: Function end\n",
    "(_::ff2)(x,y) = 2x + y\n",
    "const ff = ff2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the primitive operation here that everything really comes down\n",
    "to is calls on structs. Why is this done with unique **singleton** types? (Singleton\n",
    "types are types where every instance is equivalent). Well, if we want the compiler\n",
    "to be able to optimize with respect to which function we are handling inside of\n",
    "another function, then we need \"what function we are dealing with\" as compile-time\n",
    "information, which necessitates being type information.\n",
    "\n",
    "Tuples are contravariant and heterogeneously typed with a parameter per internal\n",
    "object. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup = (1.0,1,\"1\")\n",
    "typeof(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that it is possible to infer outputs of a tuple even if it's heterogeneously\n",
    "typed by making good use of constant literals. For example, the expression\n",
    "`tup[1]` will be inferred to have the output `Float64`. However, note that if\n",
    "`i` is not a compile-time constant, then `tup[i]` cannot be inferred since, given\n",
    "what the compiler knows, the output could be either a `Float64`, an `Int64`, or\n",
    "a `String`.\n",
    "\n",
    "So now let's think back to our tuple of functions. By what we described before,\n",
    "`tup = (f,g,h)` is going to have a different type for each of the functions and\n",
    "thus could not specialize on the inputs if we used `tup[i]`. Therefore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:length(tup)\n",
    "  x = tup[i](x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "would be slow (if the function call cost is small compared to the dispatch cost\n",
    "of about 100ns. This is not always the case, but should be considered in many\n",
    "instances!). So how can you get around it? Well, if everything was constant\n",
    "literals then this would specialize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup[3](tup[2](tup[1](x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "would fully specialize and infer, and the compiler would have full knowledge\n",
    "of the entire call chain as if it were written out as straightline code. Now\n",
    "if we look at the recursion again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applychain(::Tuple{}, x) = x\n",
    "applychain(fs::Tuple, x) = applychain(tail(fs), first(fs)(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that, at compile-time, we know that `typeof((f,g,h)) = Tuple{typeof(f),typeof(g),typeof(h)}`,\n",
    "and so we know that the first `first(fs)` will be `f`, and can specialize on this.\n",
    "We know then that `tail(fs)` has to be the `(g,h)` and so then we recurse and know\n",
    "that `g` is first and ... This means that this scheme is equivalent to have\n",
    "written out `xs[3](xs[2](xs[1](x)))` and is thus generating code perfectly specialized\n",
    "to the order and amount of functions we had put into the `Chain`. This kind of\n",
    "abstraction, an abstraction where all of the overhead compiles away, is known\n",
    "as a **zero-cost abstraction**.\n",
    "\n",
    "(Note that technically, there is a cost since the compiler has to unravel this\n",
    "chain of events.)\n",
    "\n",
    "### Training Neural Networks\n",
    "\n",
    "Now let's get into training neural networks. \"Training\" a neural network is\n",
    "simply the process of finding weights that minimize a loss function. For example,\n",
    "let's say we wanted to make our neural network be the constant function `1` for\n",
    "any input ``x \\in [0,1]^10``. We can then write the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3994.8018f0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN = Chain(Dense(10,32,tanh),\n",
    "           Dense(32,32,tanh),\n",
    "           Dense(32,5))\n",
    "loss() = sum(abs2,sum(abs2,NN(rand(10)).-1) for i in 1:100)\n",
    "loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss function takes 100 random points in `[0,1]` and then computes the output\n",
    "of the neural network minus `1` on each of the values, and sums up the squared\n",
    "values (`abs2`). Why the squared values? This means that every computed loss value\n",
    "is positive, and so we know that by decreasing the loss this means that, on average\n",
    "our neural network outputs are closer to 1. What are the weights? Since we're using\n",
    "the Flux callable struct style from above, the weights are those inside of the\n",
    "`NN` chain object, which we can inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32×10 Array{Float32,2}:\n",
       "  0.223046    0.0108541     0.335749    …  -0.328084   -0.0500545\n",
       " -0.0923871   0.0294411     0.0986074       0.142726    0.241132\n",
       " -0.131487    0.208397     -0.217594       -0.232808    0.177468\n",
       " -0.298176   -0.125423     -0.095388       -0.308862    0.0880173\n",
       " -0.173217    0.0741397     0.0670538       0.277587    0.0860211\n",
       " -0.0586145   0.291894      0.0842421   …   0.294674   -0.332403\n",
       "  0.0600897   0.0886668     0.0873822       0.0316711  -0.0413336\n",
       "  0.101654    0.276884     -0.350934       -0.253466   -0.312748\n",
       " -0.0648486   0.158078      0.173641        0.272287   -0.232527\n",
       "  0.102998   -0.0156915    -0.327317       -0.253278    0.160441\n",
       "  0.192863    0.352141     -0.273139    …  -0.0066622   0.115977\n",
       " -0.295148    0.363763     -0.359619        0.140795    0.080872\n",
       "  0.351384    0.0174734    -0.366648        0.36069    -0.363726\n",
       "  ⋮                                     ⋱              \n",
       "  0.258747   -0.0796833    -0.335894    …   0.329074   -0.0143585\n",
       "  0.107157    0.313299     -0.00433105     -0.242747   -0.16267\n",
       " -0.102998   -0.186606      0.375656       -0.360417   -0.326193\n",
       "  0.230646    0.215607     -0.310099       -0.0725064  -0.186533\n",
       "  0.195188   -0.191161      0.052459        0.222646   -0.272766\n",
       "  0.0380985   0.320844     -0.20876     …  -0.0535816   0.366029\n",
       "  0.287947   -0.136018     -0.108071        0.0828124   0.231167\n",
       "  0.293981    0.000598445   0.359672        0.23613    -0.0551567\n",
       "  0.032007   -0.203071     -0.353263        0.285805   -0.0604651\n",
       "  0.14304     0.300767     -0.117109        0.268023    0.279272\n",
       " -0.0774461   0.0136566     0.213399    …  -0.315835   -0.158687\n",
       "  0.0660572  -0.128482     -0.319713       -0.375607   -0.0879835"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN[1].W # The W matrix of the first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab all of the parameters together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[0.22304586 0.010854111 … -0.32808435 -0.050054494; -0.09238714 0.029441064 … 0.14272586 0.2411317; … ; -0.0774461 0.013656649 … -0.31583455 -0.1586868; 0.06605717 -0.12848239 … -0.37560654 -0.08798346], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.1398133 -0.028776135 … -0.16862535 0.2071802; 0.076557435 0.050060228 … -0.043571435 0.21642104; … ; 0.016137706 0.13401948 … 0.19163372 0.10837922; -0.27611703 0.2207139 … -0.026609553 -0.27622655], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.21998133 -0.030372554 … -0.086632 0.23326503; 0.13651583 -0.29353833 … 0.22584695 -0.18867786; … ; 0.30092618 -0.0560006 … -0.117234975 -0.23301877; 0.120329075 0.26528567 … 0.39914414 -0.16480075], Float32[0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = params(NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a helper function on `Chain` which recursively gathers all of the defining\n",
    "parameters. Let's not find the optimal values `p` which cause the neural network\n",
    "to be the constant `1` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(loss, p, Iterators.repeated((), 10000), ADAM(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.5545636f-6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that `NN(x)` is now a very good function approximator to `f(x) = ones(5)`!\n",
    "\n",
    "### So Why Machine Learning? Why Neural Networks?\n",
    "\n",
    "All we did was find parameters that made `NN(x)` act like a function `f(x)`.\n",
    "How does that relate to machine learning? Well, in any case where one is acting\n",
    "on data `(x,y)`, the idea is to assume that there exists some underlying\n",
    "mathematical model `f(x) = y`. If we had perfect knowledge of what `f` is, then\n",
    "from only the information of `x` we can then predict what `y` would be. The\n",
    "inference problem is to then figure out what function `f` should be. Therefore,\n",
    "machine learning on data is simply this problem of finding an approximator to\n",
    "some unknown function!\n",
    "\n",
    "So why neural networks? Neural networks satisfy two properties. The first of\n",
    "which is known as the Universal Approximation Theorem (UAT), which in simple\n",
    "non-mathematical language means that, for any ϵ of accuracy, if your neural\n",
    "network is large enough (has enough layers, the weight matrices are large enough),\n",
    "then it can approximate **any** (nice) function `f` within that ϵ. Therefore,\n",
    "we can reduce the problem of finding missing functions, the problem of machine\n",
    "learning, to a problem of finding a finding the weights of neural networks,\n",
    "which is a well-defined mathematical optimization problem.\n",
    "\n",
    "Why neural networks specifically? That's a fairly good question, since there\n",
    "are many other functions with this property. For example, you will have learned\n",
    "from analysis that ``a_0 + a_1 x + a_2 x^2 + \\ldots`` arbitrary polynomials can\n",
    "be used to approximate any analytic function (this is the Taylor series). Similarly,\n",
    "a Fourier series\n",
    "\n",
    "$$\n",
    "f(x) = a_0 + \\sum_k b_k \\cos(kx) + c_k \\sin(kx)\n",
    "$$\n",
    "\n",
    "can approximate any continuous function `f` (and discontinuous functions also\n",
    "can have convergence, etc. these are the details of a harmonic analysis course).\n",
    "\n",
    "That's all for one dimension. How about two dimensional functions? It turns out\n",
    "it's not difficult to prove that tensor products of universal approximators will\n",
    "give higher dimensional universal approximators. So for example, tensoring together\n",
    "two polynomials:\n",
    "\n",
    "$$\n",
    "a_0 + a_1 x + a_2 y + a_3 x y + a_4 x^2 y + a_5 x y^2 + a_6 x^2 y^2 + \\ldots\n",
    "$$\n",
    "\n",
    "will give a two-dimensional function approximator. But notice how we have to\n",
    "resolve every combination of terms. This means that if we used `n` coefficients\n",
    "in each dimension `d`, the total number of coefficients to build a `d`-dimensional\n",
    "universal approximator from one-dimensional objects would need ``n^d`` coefficients.\n",
    "This exponential growth is known as **the curse of dimensionality**.\n",
    "\n",
    "The second property of neural networks that makes them applicable to machine learning\n",
    "is that they overcome the curse of dimensionality. The proofs in this area\n",
    "[can be a little difficult to parse](https://arxiv.org/abs/1908.10828), but what\n",
    "they boil down to is proving in many cases that the growth of neural networks\n",
    "to sufficiently approximate a `d`-dimensional function grows as a polynomial\n",
    "of `d`, rather than exponential. This means that there's some dimensional cutoff\n",
    "where for ``d>cutoff`` it is more efficient to use a neural network. This can\n",
    "be problem-specific, but generally it tends to be the case at least by 8 or 10\n",
    "dimensions.\n",
    "\n",
    "Neural networks have a few other properties to consider as well:\n",
    "\n",
    "1. The assumptions of the neural network can be encoded into the neural architectures.\n",
    "   A neural network where the last layer has an activation function `x->x^2`\n",
    "   is a neural network where all outputs are positive. This means that if you\n",
    "   want to find a positive function, you can make the optimization easier by\n",
    "   enforcing this constraint. A lot of other constraints can be enforced, like\n",
    "   `tanh` activation functions can make the neural network be a smooth (all\n",
    "   derivatives finite) function, or other activations can cause finite numbers\n",
    "   of learnable discontinuities.\n",
    "2. Generating higher dimensional forms from one dimensional forms does not have\n",
    "   good symmetry. For example, the two-dimensional tensor Fourier basis does not\n",
    "   have a good way to represent ``sin(xy)``. This property of the approximator\n",
    "   is called (non)isotropy and more detail can be found in [this wonderful talk\n",
    "   about function approximation for multidimensional integration (cubature)](https://www.youtube.com/watch?v=JngdaWe3-gg).\n",
    "   Neural networks are naturally not aligned to a basis.\n",
    "3. Neural networks are \"easy\" to compute. There's good software for them, GPU-acceleration,\n",
    "   and all other kinds of tooling that make them particularly simple to use.\n",
    "4. There are proofs that in many scenarios for neural networks [the local\n",
    "   minima are the global minima](https://arxiv.org/abs/2006.05900), meaning that\n",
    "   local optimization is sufficient for training a neural network. Global optimization\n",
    "   (which we will cover later in the course) is much more expensive than local\n",
    "   methods like gradient descent, and thus this can be a good property to abuse\n",
    "   for faster computation.\n",
    "\n",
    "### From Machine Learning to Scientific Machine Learning: Structure and Science\n",
    "\n",
    "This understanding of a neural network and their libraries directly bridges to\n",
    "the understanding of scientific machine learning and the computation done in\n",
    "the field. In scientific machine learning, neural networks and machine learning\n",
    "are used as the basis to solve problems in scientific computing. [Scientific\n",
    "computing, as a discipline also known as Computational Science, is a field of\n",
    "study which focuses on scientific simulation, using tools such as differential\n",
    "equations to investigate physical, biological, and other phonomena](https://en.wikipedia.org/wiki/Computational_science).\n",
    "\n",
    "What we wish to do in scientific machine learning is use these properties of\n",
    "neural networks to improve the way that we investigate our scientific models.\n",
    "\n",
    "#### Aside: Why Differential Equations?\n",
    "\n",
    "Why do differential equations come up so often in as the model in the scientific\n",
    "context? This is a deep question with quite a simple answer. Essentially, all\n",
    "scientific experiments always have to test how things change. For example, you\n",
    "take a system now, you change it, and your measurement is how the changes you\n",
    "made caused changes in the system. This boils down to gather information about\n",
    "how, for some arbitrary system ``y = f(x)``, how ``\\Delta x`` is related to\n",
    "``\\Delta y``. Thus what you learn from scientific experiments, what is codified\n",
    "as scientific laws, is not \"the answer\", but the answer to how things change.\n",
    "This process of writing down equations by describing how they change precisely\n",
    "gives differential equations.\n",
    "\n",
    "## Solving ODEs with Neural Networks: The Physics-Informed Neural Network\n",
    "\n",
    "Now let's get to our first true SciML application: solving ordinary differential\n",
    "equations with neural networks. The process of solving a differential equation\n",
    "with a neural network, or using a differential equation as a regularizer in the\n",
    "loss function, is known as a **physics-informed neural network**, since this\n",
    "allows for physical equations to guide the training of the neural network in\n",
    "circumstances where data might be lacking.\n",
    "\n",
    "### Background: A Method for Solving Ordinary Differential Equations with Neural Networks\n",
    "\n",
    "[This is a result first due to Lagaris et. al from 1998](https://arxiv.org/pdf/physics/9705023.pdf).\n",
    "The idea is to solve differential equations using neural networks by\n",
    "representing the solution by a neural network and training the resulting\n",
    "network to satisfy the conditions required by the differential equation.\n",
    "\n",
    "Let's say we want to solve a system of ordinary differential equations\n",
    "\n",
    "$$u' = f(u,t)$$\n",
    "\n",
    "with $t \\in [0,1]$ and a known initial condition $u(0)=u_0$. To solve this, we\n",
    "approximate the solution by a neural network:\n",
    "\n",
    "$$NN(t) \\approx u(t)$$\n",
    "\n",
    "If $NN(t)$ was the true solution, then it would hold that $NN'(t) = f(NN(t),t)$ for\n",
    "all $t$. Thus we turn this condition into our loss function. This motivates the\n",
    "loss function:\n",
    "\n",
    "$$L(p) = \\sum_i \\left(\\frac{dNN(t_i)}{dt} - f(NN(t_i),t_i) \\right)^2$$\n",
    "\n",
    "The choice of $t_i$ could be done in many ways: it can be random, it can be a\n",
    "grid, etc. Anyways, when this loss function is minimized (gradients computed\n",
    "with standard reverse-mode automatic differentiation), then we have that\n",
    "$\\frac{dNN(t_i)}{dt} \\approx f(NN(t_i),t_i)$ and thus $NN(t)$ approximately solves\n",
    "the differential equation.\n",
    "\n",
    "Note that we still have to handle the initial condition. One simple way to do\n",
    "this is to add an initial condition term to the cost function. This would look like:\n",
    "\n",
    "$$L(p) = (NN(0) - u_0)^2 + \\sum_i \\left(\\frac{dNN(t_i)}{dt} - f(NN(t_i),t_i) \\right)^2$$\n",
    "\n",
    "While that would work, it can be more efficient to encode the initial condition\n",
    "into the function itself so that it's trivially satisfied for any possible set of\n",
    "parameters. For example, instead of directly using a neural network, we can\n",
    "use:\n",
    "\n",
    "$$g(t) = u_0 + tNN(t)$$\n",
    "\n",
    "as our solution. Notice that ``g(t)`` is thus a universal approximator for all\n",
    "continuous functions such that ``g(0)=0`` (this is a property one should prove!).\n",
    "Since ``g(t)`` will always satisfy the initial condition, we can train ``g(t)``\n",
    "to satisfy the derivative function then it will automatically be a solution to\n",
    "the derivative function. In this sense, we can use the loss function:\n",
    "\n",
    "$$L(p) = \\sum_i \\left(\\frac{dg(t_i)}{dt} - f(g(t_i),t_i) \\right)^2$$\n",
    "\n",
    "where ``p`` are the parameters that define ``g``, which in turn are the parameters\n",
    "which define the neural network ``NN`` that define ``g``. Thus this reduces down,\n",
    "once again, to simply finding weights which minimize a loss function!\n",
    "\n",
    "### Coding Up the Method\n",
    "\n",
    "Now let's implement this method with Flux. Let's define a neural network to be\n",
    "the `NN(t)` above. To make the problem easier, let's look at the ODE:\n",
    "\n",
    "$$u' = \\cos 2\\pi t$$\n",
    "\n",
    "and approximate it with the neural network from a scalar to a scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "NNODE = Chain(x -> [x], # Take in a scalar and transform it into an array\n",
    "           Dense(1,32,tanh),\n",
    "           Dense(32,1),\n",
    "           first) # Take first value, i.e. return a scalar\n",
    "NNODE(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of directly approximating the neural network, we will use the transformed\n",
    "equation that is forced to satisfy the boundary conditions. Using `u0=1.0`, we\n",
    "have the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(t) = t*NNODE(t) + 1f0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as our universal approximator. Thus, for this to be a function that satisfies\n",
    "\n",
    "$$g' = \\cos 2\\pi t$$\n",
    "\n",
    "we would need that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "ϵ = sqrt(eps(Float32))\n",
    "loss() = mean(abs2(((g(t+ϵ)-g(t))/ϵ) - cos(2π*t)) for t in 0:1f-2:1f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "would be minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Flux.Descent(0.01)\n",
    "data = Iterators.repeated((), 5000)\n",
    "iter = 0\n",
    "cb = function () #callback function to observe training\n",
    "  global iter += 1\n",
    "  if iter % 500 == 0\n",
    "    display(loss())\n",
    "  end\n",
    "end\n",
    "display(loss())\n",
    "Flux.train!(loss, Flux.params(NNODE), data, opt; cb=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well did this do? Well if we take the integral of both sides of our\n",
    "differential equation, we see it's fairly trivial:\n",
    "\n",
    "```math\n",
    "\\int g' = g = \\int \\cos 2\\pi t = C + \\frac{\\sin 2\\pi t}{2\\pi}\n",
    "```\n",
    "\n",
    "where we defined ``C = 1``. Let's take a bunch of (input,output) pairs from the\n",
    "neural network and plot it against the analytical solution to the differential\n",
    "equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "t = 0:0.001:1.0\n",
    "plot(t,g.(t),label=\"NN\")\n",
    "plot!(t,1.0 .+ sin.(2π.*t)/2π, label = \"True Solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it matches very well, and we can keep improving this fit by increasing\n",
    "the size of the neural network, using more training points, and training for\n",
    "more iterations.\n",
    "\n",
    "### Example: Harmonic Oscillator Informed Training\n",
    "\n",
    "Using this idea, differential equations encoding physical laws can be utilized\n",
    "inside of loss functions for terms which we have some basis to believe should\n",
    "approximately follow some physical system. Let's investigate this last step\n",
    "by looking at how to inform the training of a neural network using the harmonic\n",
    "oscillator.\n",
    "\n",
    "Let's assume that we are taking measurements of (position,force) in some real\n",
    "one-dimensional spring pushing and pulling against a wall.\n",
    "\n",
    "![](https://thumbs.dreamstime.com/b/hookes-law-vector-illustration-physics-extend-spring-force-explanation-scheme-compress-mathematical-experiment-weight-177188357.jpg)\n",
    "\n",
    "But instead of the simple spring, let's assume we had a more complex spring,\n",
    "for example, let's say ``F(x) = -kx + 0.1sin(x)`` where this extra term is due to\n",
    "some deformities in the medal (assume mass=1). Then by Newton's law of motion\n",
    "we have a second order ordinary differential equation:\n",
    "\n",
    "```math\n",
    "x'' = -kx + 0.1 \\sin(x)\n",
    "```\n",
    "\n",
    "We can use the [DifferentialEquations.jl package](https://diffeq.sciml.ai/stable/)\n",
    "to solve this differential equation and see what this system looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DifferentialEquations\n",
    "k = 1.0\n",
    "force(dx,x,k,t) = -k*x + 0.1sin(x)\n",
    "prob = SecondOrderODEProblem(force,1.0,0.0,(0.0,10.0),k)\n",
    "sol = solve(prob)\n",
    "plot(sol,label=[\"Velocity\" \"Position\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry if you don't understand this sytnax yet: we will go over differential\n",
    "equation solvers and DifferentialEquations.jl in a later lecture.\n",
    "\n",
    "Let's say we want to learn how to predict the force applied on the spring at\n",
    "each point in space, ``F(x)``. We want to learn a function, so this is the job\n",
    "for machine learning! However, we only have 6 measurements, which includes the\n",
    "information about (position,velocity,force) at evenly spaced times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_t = 0:0.01:10\n",
    "data_plot = sol(plot_t)\n",
    "positions_plot = [state[2] for state in data_plot]\n",
    "force_plot = [force(state[1],state[2],k,t) for state in data_plot]\n",
    "\n",
    "# Generate the dataset\n",
    "t = 0:3.3:10\n",
    "dataset = sol(t)\n",
    "position_data = [state[2] for state in sol(t)]\n",
    "force_data = [force(state[1],state[2],k,t) for state in sol(t)]\n",
    "\n",
    "plot(plot_t,force_plot,xlabel=\"t\",label=\"True Force\")\n",
    "scatter!(t,force_data,label=\"Force Measurements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we train a neural network to approximate the expected force at any location\n",
    "for this spring? To see whether this is possible with a standard neural network,\n",
    "let's just do it. Let's define a neural network to be ``F(x)`` and see if we\n",
    "can learn the force function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNForce = Chain(x -> [x],\n",
    "           Dense(1,32,tanh),\n",
    "           Dense(32,1),\n",
    "           first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our loss function will be to match the force at the (position,force) pairs\n",
    "in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss() = sum(abs2,NNForce(position_data[i]) - force_data[i] for i in 1:length(position_data))\n",
    "loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our random parameters do not do so well, so let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Flux.Descent(0.01)\n",
    "data = Iterators.repeated((), 5000)\n",
    "iter = 0\n",
    "cb = function () #callback function to observe training\n",
    "  global iter += 1\n",
    "  if iter % 500 == 0\n",
    "    display(loss())\n",
    "  end\n",
    "end\n",
    "display(loss())\n",
    "Flux.train!(loss, Flux.params(NNForce), data, opt; cb=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network almost exactly matched the dataset, but how well did it\n",
    "actually learn the real force function? Let's plot it to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_force_plot = NNForce.(positions_plot)\n",
    "\n",
    "plot(plot_t,force_plot,xlabel=\"t\",label=\"True Force\")\n",
    "plot!(plot_t,learned_force_plot,label=\"Predicted Force\")\n",
    "scatter!(t,force_data,label=\"Force Measurements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch. The problem is that a neural network can approximate any function, so it\n",
    "approximated *a* function that fits the data, but not *the correct* function.\n",
    "We somehow need to have more data... but where can we get more data?\n",
    "\n",
    "Well, even a first year undergrad in physics will know Hooke's law, which is\n",
    "that the idealized spring should satisfy ``F(x) = -kx``. This is a decent assumption\n",
    "for the evolution of the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force2(dx,x,k,t) = -k*x\n",
    "prob_simplified = SecondOrderODEProblem(force2,1.0,0.0,(0.0,10.0),k)\n",
    "sol_simplified = solve(prob_simplified)\n",
    "plot(sol,label=[\"Velocity\" \"Position\"])\n",
    "plot!(sol_simplified,label=[\"Velocity Simplified\" \"Position Simplified\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it'snot quite correct, and it definitely drifts near the end, it should\n",
    "be a useful non-data assumption that we can add to improve the fitting. So,\n",
    "assuming we know ``k`` (this lab you probably have done before!), we can\n",
    "regularize this fitting by having a term that states our neural network should\n",
    "be the solution to the differential equation.\n",
    "\n",
    "This term looks like what we had done before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_positions = [2rand()-1 for i in 1:100] # random values in [-1,1]\n",
    "loss_ode() = sum(abs2,NNForce(x) - (-k*x) for x in random_positions)\n",
    "loss_ode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this term is zero, then ``F(x) = -kx``, which is approximately true. So now\n",
    "let's put these together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 0.1\n",
    "composed_loss() = loss() + λ*loss_ode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where ``λ`` is some weight factor to control the regularization against the\n",
    "physics assumption. Now we can train the physics-informed neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Flux.Descent(0.01)\n",
    "data = Iterators.repeated((), 5000)\n",
    "iter = 0\n",
    "cb = function () #callback function to observe training\n",
    "  global iter += 1\n",
    "  if iter % 500 == 0\n",
    "    display(composed_loss())\n",
    "  end\n",
    "end\n",
    "display(composed_loss())\n",
    "Flux.train!(composed_loss, Flux.params(NNForce), data, opt; cb=cb)\n",
    "\n",
    "learned_force_plot = NNForce.(positions_plot)\n",
    "\n",
    "plot(plot_t,force_plot,xlabel=\"t\",label=\"True Force\")\n",
    "plot!(plot_t,learned_force_plot,label=\"Predicted Force\")\n",
    "scatter!(t,force_data,label=\"Force Measurements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we go: we have used knowledge of physics to help inform our neural\n",
    "network training process!\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lecture we motivated machine learning not as a process of predicting\n",
    "from data but as a process for learning arbitrary nonlinear functions. Neural\n",
    "networks were just one choice of possible function. We then demonstrated how\n",
    "differential equations could be solved using this function approximation\n",
    "technique and then put together these two domains, solving differential equations\n",
    "and approximating data, into a single process to allow for physical knowledge\n",
    "to be embedded into the training process of a neural network, thus arriving\n",
    "at a physics-informed neural network. This is just one method in scientific\n",
    "machine learning which we will be exploring in more detail, demonstrating how\n",
    "we can utilize scientific knowledge to improve fits and allow for data-efficient\n",
    "machine learning."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
